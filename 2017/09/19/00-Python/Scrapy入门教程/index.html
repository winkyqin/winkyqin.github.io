<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Scrapy入门教程 | Qin Blog</title><meta name="keywords" content="Scrapy"><meta name="author" content="WinkyQin"><meta name="copyright" content="WinkyQin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Scrapy[TOC] Scrapy入门教程1.开发准备1.1 python安装1.2 安装python IDE，PyCharm 执行jar文件   java -jar xxx.jar    1.3 macOS安装Scrapy2.1 Creating a project12345678scrapy startproject tutorial &#x2F;&#x2F;命令行创建Scrapy项目 tu">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy入门教程">
<meta property="og:url" content="http://winkyqin.com/2017/09/19/00-Python/Scrapy%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/index.html">
<meta property="og:site_name" content="Qin Blog">
<meta property="og:description" content="Scrapy[TOC] Scrapy入门教程1.开发准备1.1 python安装1.2 安装python IDE，PyCharm 执行jar文件   java -jar xxx.jar    1.3 macOS安装Scrapy2.1 Creating a project12345678scrapy startproject tutorial &#x2F;&#x2F;命令行创建Scrapy项目 tu">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2017-09-18T16:00:00.000Z">
<meta property="article:modified_time" content="2021-03-28T10:31:59.788Z">
<meta property="article:author" content="WinkyQin">
<meta property="article:tag" content="Scrapy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://winkyqin.com/2017/09/19/00-Python/Scrapy%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-03-28 18:31:59'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/null" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">208</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">63</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Qin Blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Scrapy入门教程</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2017-09-18T16:00:00.000Z" title="发表于 2017-09-19 00:00:00">2017-09-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-03-28T10:31:59.788Z" title="更新于 2021-03-28 18:31:59">2021-03-28</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Python/">Python</a></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Scrapy"><a href="#Scrapy" class="headerlink" title="Scrapy"></a>Scrapy</h1><p>[TOC]</p>
<h2 id="Scrapy入门教程"><a href="#Scrapy入门教程" class="headerlink" title="Scrapy入门教程"></a>Scrapy入门教程</h2><h3 id="1-开发准备"><a href="#1-开发准备" class="headerlink" title="1.开发准备"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/DoomHush/article/details/81056493">1.开发准备</a></h3><h4 id="1-1-python安装"><a href="#1-1-python安装" class="headerlink" title="1.1 python安装"></a>1.1 <a target="_blank" rel="noopener" href="https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/0014316090478912dab2a3a9e8f4ed49d28854b292f85bb000">python安装</a></h4><h4 id="1-2-安装python-IDE，PyCharm"><a href="#1-2-安装python-IDE，PyCharm" class="headerlink" title="1.2 安装python IDE，PyCharm"></a>1.2 <a target="_blank" rel="noopener" href="http://www.sdifen.com/pycharm201814.html">安装python IDE，PyCharm</a></h4><ul>
<li><p>执行jar文件</p>
<pre><code>  java -jar xxx.jar
</code></pre>
</li>
</ul>
<h4 id="1-3-macOS安装Scrapy"><a href="#1-3-macOS安装Scrapy" class="headerlink" title="1.3 macOS安装Scrapy"></a>1.3 <a target="_blank" rel="noopener" href="https://blog.csdn.net/u013516897/article/details/79613591">macOS安装Scrapy</a></h4><h3 id="2-1-Creating-a-project"><a href="#2-1-Creating-a-project" class="headerlink" title="2.1 Creating a project"></a>2.1 Creating a project</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject tutorial &#x2F;&#x2F;命令行创建Scrapy项目 tutorial</span><br><span class="line"></span><br><span class="line">scrapy.cfg: 项目的配置文件</span><br><span class="line">tutorial&#x2F;:  该项目的python模块。之后将在此加入代码。</span><br><span class="line">tutorial&#x2F;items.py: 项目中的item文件</span><br><span class="line">tutorial&#x2F;pipelines.py:项目中的pipelines文件.</span><br><span class="line">tutorial&#x2F;settings.py:项目中的设置文件</span><br><span class="line">tutorial&#x2F;spiders&#x2F;:放置spider代码的目录</span><br></pre></td></tr></table></figure>

<h2 id="Our-first-Spider"><a href="#Our-first-Spider" class="headerlink" title="Our first Spider"></a>Our first Spider</h2><p>Spiders是您定义的类，它用于从网站(或一组网站)中提取信息。<br>他们必须是scrapy.Spider的子类。<br>可以选择如何跟踪页面中的链接，以及如何解析下载的页面内容以提取数据。</p>
<p>这是第一个spider的代码。<br>将其保存在目录 <strong>tutorial/spiders</strong>下的py项目，名为<strong>quotes_spider.py</strong> 的文件中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class QuotesSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &quot;quotes&quot;</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        urls &#x3D; [</span><br><span class="line">            &#39;http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&#39;,</span><br><span class="line">            &#39;http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;2&#x2F;&#39;,</span><br><span class="line">        ]</span><br><span class="line">        for url in urls:</span><br><span class="line">            yield scrapy.Request(url&#x3D;url, callback&#x3D;self.parse)</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        page &#x3D; response.url.split(&quot;&#x2F;&quot;)[-2]</span><br><span class="line">        filename &#x3D; &#39;quotes-%s.html&#39; % page</span><br><span class="line">        with open(filename, &#39;wb&#39;) as f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(&#39;Saved file %s&#39; % filename)</span><br></pre></td></tr></table></figure>

<p>As you can see, our Spider subclasses  and defines some attributes and methods:<br>如您所见，scrapy.Spider 子类定义了一些属性和方法:</p>
<ul>
<li><strong>name</strong>: 用于区别Spider。名字必须唯一, 也意味着您不能为不同的Spider设置相同名称。</li>
<li><strong>start_requests()</strong>: 必须返回请求的迭代(您可以返回请求列表或编写函数方法)，Spider爬虫将从该迭代开始爬。后续的请求将从这些初始请求中依次生成。</li>
<li><strong>parse()</strong>: 是spider的一个方法。将被调用来处理为每个请求下载的响应的方法。response参数是TextResponse的一个实例，它保存了页面内容，并有进一步的方法来处理它。parse()方法通常解析响应，将爬取的数据提取为dicts（字典），并跟踪新的url，并从中创建新请求。</li>
</ul>
<h4 id="2-2-How-to-run-our-spider-（如何运行spider）"><a href="#2-2-How-to-run-our-spider-（如何运行spider）" class="headerlink" title="2.2 How to run our spider （如何运行spider）"></a>2.2 How to run our spider （如何运行spider）</h4><p>进入项目根目录，执行下列命令启动spider:</p>
<pre><code>scrapy crawl quotes
</code></pre>
<p>这个命令用我们刚刚添加的<strong>quotes</strong>名字引号运行爬行器，它将发送一些对<strong>quot.toscrape.com</strong>的请求。您将得到一个输出:</p>
<pre><code>2018-08-10 10:48:09 [scrapy.core.engine] INFO: Spider opened
...
2018-08-10 10:48:11 [scrapy.core.engine] INFO: Spider closed (finished)
</code></pre>
<p>现在，检查当前目录中的文件。<br>您应该注意到已经创建了两个新文件:quotes-1。html和quotes-2。html，以及相应url的内容，如我们的parse方法所示。</p>
<h5 id="What-just-happened-under-the-hood-hood下面发生了什么"><a href="#What-just-happened-under-the-hood-hood下面发生了什么" class="headerlink" title="What just happened under the hood? (hood下面发生了什么?)"></a>What just happened under the hood? (hood下面发生了什么?)</h5><p>Scrapy计划爬行器Spider的start_requests方法返回的<strong>scrapy.Request</strong>请求对象。在接收到每个响应之后，它实例化响应对象并调用与请求关联的回调方法(在本例中是parse方法)，将响应作为参数传递。</p>
<h4 id="A-shortcut-to-the-start-requests-method-start-requests方法的一个快捷方式"><a href="#A-shortcut-to-the-start-requests-method-start-requests方法的一个快捷方式" class="headerlink" title="A shortcut to the start_requests method(start_requests方法的一个快捷方式)"></a>A shortcut to the start_requests method(start_requests方法的一个快捷方式)</h4><p>取代用于从url请求中生成<strong>scrapy.Request</strong>对象的start_requests()实现方法。<br>，您可以只定义一个start_urls类属性和一个url列表。<br>start_requests()的默认实现将使用这个列表为您的spider创建初始请求:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class QuotesSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &quot;quotes&quot;</span><br><span class="line">    start_urls &#x3D; [</span><br><span class="line">        &#39;http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&#39;,</span><br><span class="line">        &#39;http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;2&#x2F;&#39;,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        page &#x3D; response.url.split(&quot;&#x2F;&quot;)[-2]</span><br><span class="line">        filename &#x3D; &#39;quotes-%s.html&#39; % page</span><br><span class="line">        with open(filename, &#39;wb&#39;) as f:</span><br><span class="line">            f.write(response.body)</span><br></pre></td></tr></table></figure>

<p>将调用**parse()<strong>方法来处理这些url的每个请求，尽管我们还没有明确告诉Scrapy这样做。<br>这是因为</strong>parse()**是Scrapy的默认回调方法，它在没有显式分配回调的情况下调用请求。</p>
<h4 id="Extracting-data"><a href="#Extracting-data" class="headerlink" title="Extracting data"></a>Extracting data</h4><p>学习如何使用Scrapy提取数据的最佳方法是使用shell Scrapy shell尝试选择器。<br>运行:</p>
<pre><code>scrapy shell &#39;http://quotes.toscrape.com/page/1/&#39;
</code></pre>
<ul>
<li><strong>在Windows上，需要使用双引号:</strong><pre><code>  scrapy shell &quot;http://quotes.toscrape.com/page/1/&quot;
</code></pre>
</li>
</ul>
<p>你将会看到这样的输出:</p>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[s] Available Scrapy objects:</span><br><span class="line">[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)</span><br><span class="line">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x104f0cc18&gt;</span><br><span class="line">[s]   item       &#123;&#125;</span><br><span class="line">[s]   request    &lt;GET http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&gt;</span><br><span class="line">[s]   response   &lt;200 http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&gt;</span><br><span class="line">[s]   settings   &lt;scrapy.settings.Settings object at 0x105ca4748&gt;</span><br><span class="line">[s]   spider     &lt;DefaultSpider &#39;default&#39; at 0x105f5de10&gt;</span><br><span class="line">[s] Useful shortcuts:</span><br><span class="line">[s]   fetch(url[, redirect&#x3D;True]) Fetch URL and update local objects (by default, redirects are followed)</span><br><span class="line">[s]   fetch(req)                  Fetch a scrapy.Request and update local objects </span><br><span class="line">[s]   shelp()           Shell help (print this help)</span><br><span class="line">[s]   view(response)    View response in a browser</span><br></pre></td></tr></table></figure>
</code></pre>
<p>使用shell时，可以尝试使用CSS和响应对象选择元素:</p>
<pre><code>&gt;&gt;&gt; response.css(&#39;title&#39;)
[&lt;Selector xpath=&#39;descendant-or-self::title&#39; data=&#39;&lt;title&gt;Quotes to     Scrape&lt;/title&gt;&#39;&gt;]
</code></pre>
<h5 id="XPath-a-brief-intro"><a href="#XPath-a-brief-intro" class="headerlink" title="XPath: a brief intro"></a>XPath: a brief intro</h5><h5 id="Extracting-quotes-and-authors"><a href="#Extracting-quotes-and-authors" class="headerlink" title="Extracting quotes and authors"></a>Extracting quotes and authors</h5><p>现在您已经了解了一些关于选择和提取的知识，让我们通过编写从web页面中提取引号的代码来完成爬行器。<br><a target="_blank" rel="noopener" href="http://quotes.toscrape.com/">http://quotes.toscrape.com</a> 中的每个引语都由HTML元素表示，如下所示:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class&#x3D;&quot;quote&quot;&gt;</span><br><span class="line">    &lt;span class&#x3D;&quot;text&quot;&gt;“The world as we have created it is a process of our</span><br><span class="line">    thinking. It cannot be changed without changing our thinking.”&lt;&#x2F;span&gt;</span><br><span class="line">    &lt;span&gt;</span><br><span class="line">        by &lt;small class&#x3D;&quot;author&quot;&gt;Albert Einstein&lt;&#x2F;small&gt;</span><br><span class="line">        &lt;a href&#x3D;&quot;&#x2F;author&#x2F;Albert-Einstein&quot;&gt;(about)&lt;&#x2F;a&gt;</span><br><span class="line">    &lt;&#x2F;span&gt;</span><br><span class="line">    &lt;div class&#x3D;&quot;tag&quot;&gt;</span><br><span class="line">        tag:</span><br><span class="line">        &lt;a class&#x3D;&quot;tag&quot; href&#x3D;&quot;&#x2F;tag&#x2F;change&#x2F;page&#x2F;1&#x2F;&quot;&gt;change&lt;&#x2F;a&gt;</span><br><span class="line">        &lt;a class&#x3D;&quot;tag&quot; href&#x3D;&quot;&#x2F;tag&#x2F;deep-thoughts&#x2F;page&#x2F;1&#x2F;&quot;&gt;deep-thoughts&lt;&#x2F;a&gt;</span><br><span class="line">        &lt;a class&#x3D;&quot;tag&quot; href&#x3D;&quot;&#x2F;tag&#x2F;thinking&#x2F;page&#x2F;1&#x2F;&quot;&gt;thinking&lt;&#x2F;a&gt;</span><br><span class="line">        &lt;a class&#x3D;&quot;tag&quot; href&#x3D;&quot;&#x2F;tag&#x2F;world&#x2F;page&#x2F;1&#x2F;&quot;&gt;world&lt;&#x2F;a&gt;</span><br><span class="line">    &lt;&#x2F;div&gt;</span><br><span class="line">&lt;&#x2F;div&gt;</span><br></pre></td></tr></table></figure>

<p>让我们打开scrapy shell并播放一些，以了解如何提取我们想要的数据:</p>
<pre><code>$ scrapy shell &#39;http://quotes.toscrape.com&#39;
</code></pre>
<p>我们得到了一个引用HTML元素的选择器列表:</p>
<pre><code>&gt;&gt;&gt; response.css(&quot;div.quote&quot;)
</code></pre>
<p>上面查询返回的每个选择器都允许我们对它们的子元素运行进一步的查询。<br>让我们把第一个选择器分配给一个变量，这样我们就可以直接在一个引用上运行我们的CSS选择器:</p>
<pre><code>&gt;&gt;&gt; quote = response.css(&quot;div.quote&quot;)[0]
</code></pre>
<p> 现在，让我们使用刚才创建的quote对象从引用中提取title, author和tag:   </p>
<pre><code>&gt;&gt;&gt; title = quote.css(&quot;span.text::text&quot;).extract_first()
&gt;&gt;&gt; title
&#39;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&#39;
&gt;&gt;&gt; author = quote.css(&quot;small.author::text&quot;).extract_first()
&gt;&gt;&gt; author
&#39;Albert Einstein&#39;
</code></pre>
<p>假设标记是字符串列表，我们可以使用.extract()方法获得所有字符串:</p>
<pre><code>&gt;&gt;&gt; tag = quote.css(&quot;div.tag a.tag::text&quot;).extract()
&gt;&gt;&gt; tag
[&#39;change&#39;, &#39;deep-thoughts&#39;, &#39;thinking&#39;, &#39;world&#39;]
</code></pre>
<p>知道了如何提取每一个位之后，我们现在可以遍历所有的引号元素，并将它们放到Python字典中:</p>
<pre><code>&gt;&gt;&gt; for quote in response.css(&quot;div.quote&quot;):
...     text = quote.css(&quot;span.text::text&quot;).extract_first()
...     author = quote.css(&quot;small.author::text&quot;).extract_first()
...     tag = quote.css(&quot;div.tag a.tag::text&quot;).extract()
...     print(dict(text=text, author=author, tag=tag))
&#123;&#39;tag&#39;: [&#39;change&#39;, &#39;deep-thoughts&#39;, &#39;thinking&#39;, &#39;world&#39;], &#39;author&#39;: &#39;Albert Einstein&#39;, &#39;text&#39;: &#39;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&#39;&#125;
&#123;&#39;tag&#39;: [&#39;abilities&#39;, &#39;choices&#39;], &#39;author&#39;: &#39;J.K. Rowling&#39;, &#39;text&#39;: &#39;“It is our choices, Harry, that show what we truly are, far more than our abilities.”&#39;&#125;
    ... a few more of these, omitted for brevity
&gt;&gt;&gt;
</code></pre>
<h4 id="Extracting-data-in-our-spider-（提取爬虫数据）"><a href="#Extracting-data-in-our-spider-（提取爬虫数据）" class="headerlink" title="Extracting data in our spider （提取爬虫数据）"></a>Extracting data in our spider （提取爬虫数据）</h4><h3 id="Storing-the-scraped-data-（存储爬取到数据）"><a href="#Storing-the-scraped-data-（存储爬取到数据）" class="headerlink" title="Storing the scraped data （存储爬取到数据）"></a>Storing the scraped data （存储爬取到数据）</h3><p>存储剪贴数据的最简单方法是使用提要导出，使用以下命令:</p>
<pre><code>scrapy crawl quotes -o quotes.json
</code></pre>
<p>这会产生一个引号。json文件，包含所有剪贴项，用json序列化。<br>出于历史原因，Scrapy将一个给定的文件附加而不是覆盖其内容。如果您在第二次运行该命令两次而没有在第二次之前删除该文件，那么最终会得到一个损坏的JSON文件。</p>
<p>还可以使用其他格式，比如JSON行:</p>
<pre><code>scrapy crawl quotes -o quotes.jl
</code></pre>
<p>JSON行格式很有用，因为它类似于流，您可以很容易地向其追加新记录。当你运行两次时，它不会有JSON那样的问题。另外，由于每个记录都是单独的一行，您可以处理大文件，而不必将所有内容都放在内存中。   </p>
<p>在小项目中(如本教程中的项目)，这就足够了。但是，如果您想要对已清理的项执行更复杂的操作，您可以编写一个项管道。<br>在tutorial/pipelines.py中，项目创建时为您设置了项目管道的占位符文件。<br>不过，如果您只是想要存储剪贴件，则不需要实现任何项管道。 </p>
<h3 id="Following-Links"><a href="#Following-Links" class="headerlink" title="Following Links"></a>Following Links</h3><h4 id="A-shortcut-for-creating-Requests"><a href="#A-shortcut-for-creating-Requests" class="headerlink" title="A shortcut for creating Requests"></a>A shortcut for creating Requests</h4><h4 id="More-examples-and-patterns"><a href="#More-examples-and-patterns" class="headerlink" title="More examples and patterns"></a>More examples and patterns</h4><h3 id="Using-spider-arguments"><a href="#Using-spider-arguments" class="headerlink" title="Using spider arguments"></a>Using spider arguments</h3><h3 id="Next-steps"><a href="#Next-steps" class="headerlink" title="Next steps"></a>Next steps</h3><h1 id="Scrapy爬虫添加MySQL支持"><a href="#Scrapy爬虫添加MySQL支持" class="headerlink" title="Scrapy爬虫添加MySQL支持"></a>Scrapy爬虫添加MySQL支持</h1><h2 id="1-Mac添加MySQL支持"><a href="#1-Mac添加MySQL支持" class="headerlink" title="1. Mac添加MySQL支持"></a>1. Mac添加MySQL支持</h2><h3 id="1-1-mysql官网下载并安装"><a href="#1-1-mysql官网下载并安装" class="headerlink" title="1.1 mysql官网下载并安装"></a>1.1 mysql官网下载并安装</h3><ul>
<li>添加zsh命令行.bash_profile支持<pre><code>  sudo vim .bash_profile
  //追加一行 /usr/local/mysql为mysql默认安装目录
  export PATH=/usr/local/mysql/bin:$PATH
  //保存并更新
  source ~/.bash_profile
</code></pre>
</li>
</ul>
<h3 id="1-2-mysql命令行使用"><a href="#1-2-mysql命令行使用" class="headerlink" title="1.2 mysql命令行使用"></a>1.2 mysql命令行使用</h3><pre><code>//默认找到bin文件目录
/usr/local/mysql/bin/mysql -u root -p
//配置.bash_profile后直接执行
mysql -u root -p
</code></pre>
<h3 id="1-3-MySQL数据库连接并操作Python3（PyMySQL驱动）"><a href="#1-3-MySQL数据库连接并操作Python3（PyMySQL驱动）" class="headerlink" title="1.3 MySQL数据库连接并操作Python3（PyMySQL驱动）"></a>1.3 MySQL数据库连接并操作Python3（PyMySQL驱动）</h3><h5 id="1-sql查询操作"><a href="#1-sql查询操作" class="headerlink" title="1. sql查询操作"></a>1. sql查询操作</h5><pre><code>&#39;&#39;&#39;    
# coding=utf-8

import pymysql

if __name__ == &#39;__main__&#39;:
# 1. 打开数据库连接
connection = pymysql.connect(host=&quot;localhost&quot;, user=&quot;root&quot;,
                             password=&quot;91499419&quot;, db=&quot;database_chain_news&quot;, port=3306)

# 2. 获取游标对象
cursor = connection.cursor()

&#39;&#39;&#39;
# query version 
sql = &quot;select version()&quot;
cursor.execute(sql)
data = cursor.fetchone()
print(data)
&#39;&#39;&#39;

# 3. 数据库创建和删除

## 判断数据库不存在，创建数据库
# sql_create_db = &quot;create database If Not Exists database_chain_news charset UTF8;&quot;
# cursor.execute(sql_create_db)

# sql_use_db = &quot;use database_chain_news;&quot;
# cursor.execute(sql_use_db)

## 判断数据库存在, 则删除:
# sql_drop_db = &quot;drop database if exists database_name;&quot;

# 4. 数据表的创建和删除
## 数据表不存在，直接创建表
try:
    # sql_create_table = &quot;create table if not exists tl_babtc_flash(babtc_id int primary key , source varchar(100) , title varchar(100) , babtc_content varchar(100) , babtc_post_date int, babtc_views int, babtc_post_name varchar(100) , babtc_desc varchar(100));&quot;
    #         # cursor.execute(sql_create_table)

    # 5. 执行sql查询操作

    # 6. 使用fetchone() 获取单条数据

    # 7. 表插入操作
    ### 1. 在 Python 中使用 sqlite3 连接数据库，插入语句的展位符为 &quot;？&quot;
    ### cur.execute(&quot;insert into user values(?,?,?)&quot;,(1,2,&quot;zhang&quot;))

    ### 2. 在 Python 中，使用 pymysql 连接 mysql 数据库，插入语句的占位符为 &quot;%s&quot;
    ### cur.execute(&quot;insert into user values(?,?,?)&quot;, (1, 2, &quot;zhang&quot;))

    sql_insert_flash = &quot;insert into tl_babtc_flash(babtc_id, source, title, babtc_content, babtc_post_date, &quot; \
                       &quot;babtc_views, babtc_post_name, babtc_desc)&quot; \
                       &quot; value (%s, %s, %s, %s, %s, %s, %s, %s);&quot;
    cursor.execute(sql_insert_flash, (1, &quot;source&quot;, &quot;title&quot;, &quot;content&quot;, 100, 99, &quot;post_name&quot;, &quot;desc&quot;))
    connection.commit()
# 8. 表更新操作

# 9. 表查询操作

# 10. 表删除操作

# 11. 事务处理
except:
    connection.rollback()
# 12. 异常处理
finally:
    connection.close()
# 13. 关闭连接
</code></pre>
<p>‘’’</p>
<h2 id="2-Ubuntu服务器端"><a href="#2-Ubuntu服务器端" class="headerlink" title="2. Ubuntu服务器端"></a>2. Ubuntu服务器端</h2><p>coding=utf-8</p>
<p>import pymysql</p>
<p>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>    # 1. 打开数据库连接<br>    connection = pymysql.connect(host=”localhost”, user=”root”,<br>                                 password=”91499419”, db=”database_chain_news”, port=3306)</p>
<pre><code># 2. 获取游标对象
cursor = connection.cursor()

&#39;&#39;&#39;
# query version 
sql = &quot;select version()&quot;
cursor.execute(sql)
data = cursor.fetchone()
print(data)
&#39;&#39;&#39;

# 3. 数据库创建和删除

## 判断数据库不存在，创建数据库
# sql_create_db = &quot;create database If Not Exists database_chain_news charset UTF8;&quot;
# cursor.execute(sql_create_db)

# sql_use_db = &quot;use database_chain_news;&quot;
# cursor.execute(sql_use_db)

## 判断数据库存在, 则删除:
# sql_drop_db = &quot;drop database if exists database_name;&quot;

# 4. 数据表的创建和删除
## 数据表不存在，直接创建表
try:
    # sql_create_table = &quot;create table if not exists tl_babtc_flash(babtc_id int primary key , source varchar(100) , title varchar(100) , babtc_content varchar(100) , babtc_post_date int, babtc_views int, babtc_post_name varchar(100) , babtc_desc varchar(100));&quot;
    #         # cursor.execute(sql_create_table)

    # 5. 执行sql查询操作

    # 6. 使用fetchone() 获取单条数据

    # 7. 表插入操作
    ### 1. 在 Python 中使用 sqlite3 连接数据库，插入语句的展位符为 &quot;？&quot;
    ### cur.execute(&quot;insert into user values(?,?,?)&quot;,(1,2,&quot;zhang&quot;))

    ### 2. 在 Python 中，使用 pymysql 连接 mysql 数据库，插入语句的占位符为 &quot;%s&quot;
    ### cur.execute(&quot;insert into user values(?,?,?)&quot;, (1, 2, &quot;zhang&quot;))

    sql_insert_flash = &quot;insert into tl_babtc_flash(babtc_id, source, title, babtc_content, babtc_post_date, &quot; \
                       &quot;babtc_views, babtc_post_name, babtc_desc)&quot; \
                       &quot; value (%s, %s, %s, %s, %s, %s, %s, %s);&quot;
    cursor.execute(sql_insert_flash, (1, &quot;source&quot;, &quot;title&quot;, &quot;content&quot;, 100, 99, &quot;post_name&quot;, &quot;desc&quot;))
    connection.commit()
# 8. 表更新操作

# 9. 表查询操作

# 10. 表删除操作

# 11. 事务处理
except:
    connection.rollback()
# 12. 异常处理
finally:
    connection.close()
# 13. 关闭连接
</code></pre>
<h1 id="Scrapy-链接"><a href="#Scrapy-链接" class="headerlink" title="Scrapy 链接"></a>Scrapy 链接</h1><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/23b6d6b57ec5">01-基础知识</a><br> <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/43029ea38251">Scrapy爬虫入门教程一 安装和基本使用</a></p>
<p><a target="_blank" rel="noopener" href="https://doc.scrapy.org/en/latest/index.html">scrapy官方教程</a></p>
<p><a target="_blank" rel="noopener" href="https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/items.html">scrapy中文官方教程</a></p>
<p><a target="_blank" rel="noopener" href="http://api.mongodb.com/python/current/tutorial.html">pymongo tutorial</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u012052268/article/details/72063917"># pycharm下打开、执行并调试scrapy爬虫程序</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/dad7c2c51c94">python3 爬取区块链okb网行情图表数据</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/xinyangsdut/p/7628770.html">Python爬虫框架Scrapy实例（一）</a></p>
<p><strong>目标任务：</strong></p>
<ul>
<li>爬取腾讯社招信息</li>
</ul>
<p><strong>爬取内容：</strong></p>
<ul>
<li>职位名称，职位的详情链接，职位类别，招聘人数，工作地点，发布时间。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/xinyangsdut/p/7631163.html">Python爬虫框架Scrapy实例（二）</a></p>
<p><strong>目标任务：</strong></p>
<ul>
<li>使用Scrapy框架爬取新浪网导航页所有大类、小类、小类里的子链接、以及子链接页面的新闻内容，最后保存到本地。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/freeman818/p/7228976.html">scrapy爬虫事件以及数据保存为txt,json,mysql</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/lei0213/p/7900340.html">python爬虫scrapy之如何同时执行多个scrapy爬行任务</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/jinxiao-pu/p/6762636.html">第7章 Scrapy突破反爬虫的限制</a> </p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/zhanghongfeng/p/7684415.html">python网络爬虫之使用scrapy自动登录网站</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/seven_2016/article/details/72802961">基于Scrapy分布式爬虫的开发与设计</a></p>
<h1 id="Scrapy部署"><a href="#Scrapy部署" class="headerlink" title="Scrapy部署"></a>Scrapy部署</h1><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/93ccb59ce9b3">Scrapyd 部署</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zhaobig/article/details/78670176">Scrapyd 部署爬虫项目</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lowerxiaoshen/article/details/78534925">ubuntu下 scrapyd部署爬虫项目</a>    </p>
<h1 id="python爬虫scrapy命令工具学习之篇三"><a href="#python爬虫scrapy命令工具学习之篇三" class="headerlink" title="python爬虫scrapy命令工具学习之篇三"></a>python爬虫scrapy命令工具学习之篇三</h1><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/maxiaofang/p/6691498.html">python爬虫scrapy命令工具学习之篇三</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">WinkyQin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://winkyqin.com/2017/09/19/00-Python/Scrapy%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/">http://winkyqin.com/2017/09/19/00-Python/Scrapy%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://winkyqin.com" target="_blank">Qin Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Scrapy/">Scrapy</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2017/09/19/00-Python/%E5%8C%BA%E5%9D%97%E9%93%BE%E8%B5%84%E8%AE%AF%E6%95%B0%E6%8D%AE%E7%88%AC%E8%99%AB%E8%A7%84%E5%88%92/"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">区块链资讯数据爬虫规划</div></div></a></div><div class="next-post pull-right"><a href="/2017/09/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E4%B9%89/Python%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-00/"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Python自然语言处理</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2017/10/21/03-Python高级/D_PythonHigh_20_爬虫Scrapy框架及案例/" title="Scrapy框架"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2017-10-21</div><div class="title">Scrapy框架</div></div></a></div><div><a href="/2017/09/19/Ubuntu/Ubuntu-scrapy-clawer/" title="Ubuntu服务器部署"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2017-09-19</div><div class="title">Ubuntu服务器部署</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/null" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">WinkyQin</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">208</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">63</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/winkyqin"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/winkyqin" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://www.zhihu.com/people/winkyqin" target="_blank" title="Zhihu"><i class="fab fa-zhihu"></i></a><a class="social-icon" href="https://www.linkedin.com/in/winkyqin/" target="_blank" title="Linkedin"><i class="fab fa-linkedin"></i></a><a class="social-icon" href="mailto:winkyqin@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">Have a nice day!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Scrapy"><span class="toc-number">1.</span> <span class="toc-text">Scrapy</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B"><span class="toc-number">1.1.</span> <span class="toc-text">Scrapy入门教程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%BC%80%E5%8F%91%E5%87%86%E5%A4%87"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.开发准备</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-python%E5%AE%89%E8%A3%85"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">1.1 python安装</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E5%AE%89%E8%A3%85python-IDE%EF%BC%8CPyCharm"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">1.2 安装python IDE，PyCharm</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-macOS%E5%AE%89%E8%A3%85Scrapy"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">1.3 macOS安装Scrapy</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Creating-a-project"><span class="toc-number">1.1.2.</span> <span class="toc-text">2.1 Creating a project</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Our-first-Spider"><span class="toc-number">1.2.</span> <span class="toc-text">Our first Spider</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-How-to-run-our-spider-%EF%BC%88%E5%A6%82%E4%BD%95%E8%BF%90%E8%A1%8Cspider%EF%BC%89"><span class="toc-number">1.2.0.1.</span> <span class="toc-text">2.2 How to run our spider （如何运行spider）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#What-just-happened-under-the-hood-hood%E4%B8%8B%E9%9D%A2%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88"><span class="toc-number">1.2.0.1.1.</span> <span class="toc-text">What just happened under the hood? (hood下面发生了什么?)</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#A-shortcut-to-the-start-requests-method-start-requests%E6%96%B9%E6%B3%95%E7%9A%84%E4%B8%80%E4%B8%AA%E5%BF%AB%E6%8D%B7%E6%96%B9%E5%BC%8F"><span class="toc-number">1.2.0.2.</span> <span class="toc-text">A shortcut to the start_requests method(start_requests方法的一个快捷方式)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Extracting-data"><span class="toc-number">1.2.0.3.</span> <span class="toc-text">Extracting data</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#XPath-a-brief-intro"><span class="toc-number">1.2.0.3.1.</span> <span class="toc-text">XPath: a brief intro</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Extracting-quotes-and-authors"><span class="toc-number">1.2.0.3.2.</span> <span class="toc-text">Extracting quotes and authors</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Extracting-data-in-our-spider-%EF%BC%88%E6%8F%90%E5%8F%96%E7%88%AC%E8%99%AB%E6%95%B0%E6%8D%AE%EF%BC%89"><span class="toc-number">1.2.0.4.</span> <span class="toc-text">Extracting data in our spider （提取爬虫数据）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Storing-the-scraped-data-%EF%BC%88%E5%AD%98%E5%82%A8%E7%88%AC%E5%8F%96%E5%88%B0%E6%95%B0%E6%8D%AE%EF%BC%89"><span class="toc-number">1.2.1.</span> <span class="toc-text">Storing the scraped data （存储爬取到数据）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Following-Links"><span class="toc-number">1.2.2.</span> <span class="toc-text">Following Links</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#A-shortcut-for-creating-Requests"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">A shortcut for creating Requests</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#More-examples-and-patterns"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">More examples and patterns</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Using-spider-arguments"><span class="toc-number">1.2.3.</span> <span class="toc-text">Using spider arguments</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Next-steps"><span class="toc-number">1.2.4.</span> <span class="toc-text">Next steps</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Scrapy%E7%88%AC%E8%99%AB%E6%B7%BB%E5%8A%A0MySQL%E6%94%AF%E6%8C%81"><span class="toc-number">2.</span> <span class="toc-text">Scrapy爬虫添加MySQL支持</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Mac%E6%B7%BB%E5%8A%A0MySQL%E6%94%AF%E6%8C%81"><span class="toc-number">2.1.</span> <span class="toc-text">1. Mac添加MySQL支持</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-mysql%E5%AE%98%E7%BD%91%E4%B8%8B%E8%BD%BD%E5%B9%B6%E5%AE%89%E8%A3%85"><span class="toc-number">2.1.1.</span> <span class="toc-text">1.1 mysql官网下载并安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-mysql%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BD%BF%E7%94%A8"><span class="toc-number">2.1.2.</span> <span class="toc-text">1.2 mysql命令行使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E5%B9%B6%E6%93%8D%E4%BD%9CPython3%EF%BC%88PyMySQL%E9%A9%B1%E5%8A%A8%EF%BC%89"><span class="toc-number">2.1.3.</span> <span class="toc-text">1.3 MySQL数据库连接并操作Python3（PyMySQL驱动）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-sql%E6%9F%A5%E8%AF%A2%E6%93%8D%E4%BD%9C"><span class="toc-number">2.1.3.0.1.</span> <span class="toc-text">1. sql查询操作</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Ubuntu%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF"><span class="toc-number">2.2.</span> <span class="toc-text">2. Ubuntu服务器端</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Scrapy-%E9%93%BE%E6%8E%A5"><span class="toc-number">3.</span> <span class="toc-text">Scrapy 链接</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Scrapy%E9%83%A8%E7%BD%B2"><span class="toc-number">4.</span> <span class="toc-text">Scrapy部署</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#python%E7%88%AC%E8%99%ABscrapy%E5%91%BD%E4%BB%A4%E5%B7%A5%E5%85%B7%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%AF%87%E4%B8%89"><span class="toc-number">5.</span> <span class="toc-text">python爬虫scrapy命令工具学习之篇三</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2021/02/25/interview/job_python_engineer/" title="个人简历-危勤">个人简历-危勤</a><time datetime="2021-02-24T16:00:00.000Z" title="发表于 2021-02-25 00:00:00">2021-02-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2020/07/27/Others/Redis%E4%BB%A4%E7%89%8C%E6%9C%BA%E5%88%B6%E5%AE%9E%E7%8E%B0%E5%95%86%E5%9F%8E%E7%A7%92%E6%9D%80/" title="Redis令牌机制实现商城秒杀">Redis令牌机制实现商城秒杀</a><time datetime="2020-07-26T21:29:32.000Z" title="发表于 2020-07-27 05:29:32">2020-07-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2020/03/28/Others/Python%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%9F%BA%E7%A1%80/" title="Python数据挖掘基础">Python数据挖掘基础</a><time datetime="2020-03-28T14:35:08.000Z" title="发表于 2020-03-28 22:35:08">2020-03-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2020/02/27/Others/Docker%E5%AE%B9%E5%99%A8%E4%BD%BF%E7%94%A8/" title="Docker容器使用">Docker容器使用</a><time datetime="2020-02-27T08:55:31.000Z" title="发表于 2020-02-27 16:55:31">2020-02-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2020/02/27/interview/%E5%AF%8C%E9%80%94python%E9%9D%A2%E8%AF%95/" title="富途python面试">富途python面试</a><time datetime="2020-02-27T08:55:31.000Z" title="发表于 2020-02-27 16:55:31">2020-02-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By WinkyQin</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div></div></body></html>